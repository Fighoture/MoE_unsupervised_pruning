{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd8d9c22-a112-490c-86d8-ade437c692f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from sklearn.cluster import KMeans\n",
    "from data_utils import dataset_local_load\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "504b47a9-a5a6-4aff-bd2c-e556cd1284ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:181: UserWarning: flash_attn is needed, but it is not supported in mac system. You can only use normal attn implementation\n",
      "  warnings.warn(f\"flash_attn is needed, but it is not supported in mac system. You can only use normal attn implementation\", UserWarning)\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/Users/jamesyao/.cache/modelscope/hub/deepseek-ai/DeepSeek-V2-Lite\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "941ca356-592a-4302-8309-61c7bc81ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"dataset\"\n",
    "train_dataset_map, valid_dataset_map = dataset_local_load(dataset_dir)\n",
    "dataset_name = \"MathInstruct\"\n",
    "train_dataset = train_dataset_map[dataset_name]\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "train_df = train_df.sample(n=5, random_state=1, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7d304-f19c-40ae-879c-1b72a6c0cebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsupervising...\n",
      "feed forward finish.\n",
      "layer 1 feed forward finish.\n",
      "layer 1 expert forward finish.\n",
      "layer 2 feed forward finish.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 2 expert forward finish.\n",
      "layer 3 feed forward finish.\n",
      "layer 3 expert forward finish.\n",
      "layer 4 feed forward finish.\n"
     ]
    }
   ],
   "source": [
    "class PreTrainedDeepseekV2PrunerByDomain:\n",
    "    def __init__(self, model, calibration_data):\n",
    "        self.model = model.model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.unsupervised_method = KMeans(n_clusters=model.config.num_experts_per_tok, random_state=0)\n",
    "        self.calibration_data = calibration_data\n",
    "\n",
    "    def generate_unsupervised_map(self):\n",
    "        print(\"unsupervising...\")\n",
    "        prompt = list(self.calibration_data[\"prompt\"])\n",
    "        completion = list(self.calibration_data[\"completion\"])\n",
    "        inputs = self.tokenizer(prompt, completion, return_tensors='pt', max_length=128, padding=True, truncation=True)\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        output = self.model(input_ids, attention_mask, output_hidden_states=True, pre_ffn_hidden=True)\n",
    "        print(\"feed forward finish.\")\n",
    "\n",
    "        pre_ffn_hidden_states = output.pre_ffn_hidden_states\n",
    "        assert len(pre_ffn_hidden_states) == len(self.model.layers)\n",
    "\n",
    "        self.unsupervised_map = {}\n",
    "        for idx, hidden_state in enumerate(pre_ffn_hidden_states):\n",
    "            if \"DeepseekV2MLP\" in str(type(self.model.layers[idx].mlp)):\n",
    "                continue\n",
    "            \n",
    "            score_weight = self.model.layers[idx].mlp.gate.weight\n",
    "            scores = F.linear(hidden_state.type(torch.float32), score_weight.type(torch.float32), None)\n",
    "            scores = scores.softmax(dim=-1, dtype=torch.float32).sum(0).sum(0).tolist()\n",
    "            print(f\"layer {idx} feed forward finish.\")\n",
    "\n",
    "            experts = self.model.layers[idx].mlp.experts\n",
    "            experts_output = []\n",
    "            for expert in experts:\n",
    "                basic_output = expert(hidden_state).sum(1).flatten().type(torch.float16)\n",
    "                experts_output.append(basic_output)\n",
    "            print(f\"layer {idx} expert forward finish.\")\n",
    "\n",
    "            experts_output = torch.stack(experts_output)\n",
    "            kmeans_result = self.unsupervised_method.fit(experts_output.detach().numpy())\n",
    "            cluster_label = kmeans_result.labels_.tolist()\n",
    "            assert len(cluster_label) == self.model.config.n_routed_experts\n",
    "\n",
    "            self.unsupervised_map[idx] = {cluster_label[i]:scores[i] for i in range(len(cluster_label))}\n",
    "\n",
    "    def generate_pruned_map(self, prune_rate=0.5):\n",
    "        print(\"pruning...\")\n",
    "        if self.unsupervised_map is None:\n",
    "            raise ValueError(\"No existed unsupervised map.\")\n",
    "        self.pruned_map = {}\n",
    "        for layer_idx, cluster_info in self.unsupervised_map.items():\n",
    "            new_map = {}\n",
    "            for expert_idx, (cluster_label, score) in enumerate(cluster_info):\n",
    "                if cluster_label not in new_map:\n",
    "                    new_map[cluster_label] = {}\n",
    "                new_map[cluster_label][expert_idx] = score\n",
    "\n",
    "            for cluster_label, cluster_info in new_map.items():\n",
    "                cluster_length = len(cluster_info)\n",
    "                if cluster_length > 1:\n",
    "                    cluster_pruned_info = sorted(cluster_info.items(), key=lambda x: x[1])[:int(prune_rate * cluster_length)]\n",
    "                    if layer_idx not in self.pruned_map:\n",
    "                        self.pruned_map[layer_idx] = []\n",
    "                    self.pruned_map[layer_idx] += [info[0] for info in cluster_pruned_info]\n",
    "                    \n",
    "\n",
    "pruner = PreTrainedDeepseekV2PrunerByDomain(model, train_df)\n",
    "pruner.generate_unsupervised_map()\n",
    "pruner.generate_pruned_map()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3635bce-9917-453d-a326-c526d6968f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.model(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60331239-6db6-41e2-bc06-09e33efea7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nA train is moving at a speed of 90km/hr and its length is 500m. Find the time taken by it to pass a man standing near the railway line?\\nAnswer Choices: (A) 30sec (B) 45sec (C) 36sec (D) 20sec (E) 52sec Let's write a program.\\n\\n### Response:# convert speed from km/hr to m/sec\\nspeed = 90 * 1000 / 3600\\n# calculate time taken to pass the man\\ntime = 500 / speed\\nprint(time)\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer test\n",
    "inputs = tokenizer(list(train_df[\"prompt\"])[:10], list(train_df[\"completion\"])[:10], return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "tokenizer.decode(input_ids.tolist()[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0482a0b9-f2b6-46b8-9164-178d588f1765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is a scalar, which is a number. The query and keys are vectors, and the values are vectors. The attention function is used to compute the output from the query and keys. The attention function is used in many different applications, such as natural language processing, computer vision, and recommendation systems.\\nThe attention function is a mathematical function that is used to compute the output of a neural network. The function is used to compute the output of a neural network by taking into account the input data and the'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text generate test\n",
    "text = \"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de994cec-6ed2-438d-abca-250c9e298301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
